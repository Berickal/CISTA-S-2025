{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Réseaux de Neurones Récurrents (RNN)\n",
    "\n",
    "## Objectifs\n",
    "- Comprendre les concepts fondamentaux des RNN (Simple RNN, LSTM, GRU)\n",
    "- Implémenter des modèles RNN pour l'analyse de sentiment\n",
    "- Comparer les performances des différents types de RNN\n",
    "- Visualiser le processus d'apprentissage\n",
    "- Comprendre les applications pratiques des RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et Configuration\n",
    "\n",
    "Importation des bibliothèques nécessaires pour les RNN avec Keras/TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (2.3.4)\n",
      "Requirement already satisfied: pandas in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: seaborn in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (6.33.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: pillow in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from seaborn) (3.10.7)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/euraste.djire/Documents/Python/venv/default/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow numpy pandas seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "\n",
    "# Style pour les graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Vérifier la disponibilité du GPU\n",
    "print(f\"GPU disponible: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Paramètres globaux\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction aux RNN\n",
    "\n",
    "### 2.1. Concepts Fondamentaux\n",
    "\n",
    "Les **Réseaux de Neurones Récurrents (RNN)** sont conçus pour traiter des données séquentielles (texte, séries temporelles, audio, etc.).\n",
    "\n",
    "**Caractéristiques principales :**\n",
    "- **Mémoire** : Les RNN maintiennent une mémoire des informations précédentes dans la séquence\n",
    "- **Paramètres partagés** : Les mêmes poids sont utilisés à chaque pas de temps\n",
    "- **Propagation dans le temps** : L'information circule à travers les pas de temps\n",
    "\n",
    "**Types de RNN :**\n",
    "1. **Simple RNN** : Architecture de base, mais souffre du problème de gradient qui disparaît\n",
    "2. **LSTM (Long Short-Term Memory)** : Résout le problème du gradient qui disparaît avec des portes (forget, input, output)\n",
    "3. **GRU (Gated Recurrent Unit)** : Variante simplifiée de LSTM avec moins de paramètres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Application Pratique : Analyse de Sentiment\n",
    "\n",
    "Dans ce TP, nous allons utiliser les RNN pour classer le sentiment de critiques de films comme **positif** ou **négatif**.\n",
    "\n",
    "**Dataset** : Nous utiliserons le dataset IMDb (Internet Movie Database) disponible dans Keras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement et Préparation des Données\n",
    "\n",
    "### 3.1. Chargement du Dataset IMDb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset IMDb\n",
    "print(\"Chargement du dataset IMDb...\")\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=10000)\n",
    "\n",
    "print(f\"Nombre d'échantillons d'entraînement: {len(x_train)}\")\n",
    "print(f\"Nombre d'échantillons de test: {len(x_test)}\")\n",
    "print(f\"\\nExemple de séquence (premiers 20 indices): {x_train[0][:20]}\")\n",
    "print(f\"Label (0 = négatif, 1 = positif): {y_train[0]}\")\n",
    "\n",
    "# Statistiques sur les séquences\n",
    "train_lengths = [len(seq) for seq in x_train]\n",
    "test_lengths = [len(seq) for seq in x_test]\n",
    "\n",
    "print(f\"\\nLongueur moyenne des séquences d'entraînement: {np.mean(train_lengths):.2f}\")\n",
    "print(f\"Longueur max des séquences d'entraînement: {max(train_lengths)}\")\n",
    "print(f\"Longueur min des séquences d'entraînement: {min(train_lengths)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Décodage des Textes (optionnel)\n",
    "\n",
    "Décodons quelques exemples pour voir les textes originaux.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dictionnaire des mots (index -> mot)\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# Créer un dictionnaire inverse (index -> mot)\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "reverse_word_index[0] = \"<PAD>\"  # Padding\n",
    "reverse_word_index[1] = \"<START>\"  # Start of sequence\n",
    "reverse_word_index[2] = \"<UNK>\"  # Unknown word\n",
    "\n",
    "def decode_review(encoded_review):\n",
    "    \"\"\"Décode une séquence d'indices en texte\"\"\"\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in encoded_review])\n",
    "\n",
    "# Afficher quelques exemples\n",
    "print(\"Exemple 1 (Label: {}):\".format(\"Positif\" if y_train[0] == 1 else \"Négatif\"))\n",
    "print(decode_review(x_train[0][:50]))  # Premiers 50 mots\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Exemple 2 (Label: {}):\".format(\"Positif\" if y_train[1] == 1 else \"Négatif\"))\n",
    "print(decode_review(x_train[1][:50]))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Exemple 3 (Label: {}):\".format(\"Positif\" if y_train[2] == 1 else \"Négatif\"))\n",
    "print(decode_review(x_train[2][:50]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Padding et Truncation des Séquences\n",
    "\n",
    "Les RNN nécessitent des séquences de longueur fixe. Nous allons :\n",
    "- **Padding** : Ajouter des zéros aux séquences courtes\n",
    "- **Truncation** : Tronquer les séquences longues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres de preprocessing\n",
    "MAX_LENGTH = 500  # Longueur maximale des séquences\n",
    "VOCAB_SIZE = 10000  # Taille du vocabulaire\n",
    "\n",
    "# Padding et truncation\n",
    "x_train_padded = pad_sequences(x_train, maxlen=MAX_LENGTH, padding='pre', truncating='pre')\n",
    "x_test_padded = pad_sequences(x_test, maxlen=MAX_LENGTH, padding='pre', truncating='pre')\n",
    "\n",
    "print(f\"Shape des données d'entraînement après padding: {x_train_padded.shape}\")\n",
    "print(f\"Shape des données de test après padding: {x_test_padded.shape}\")\n",
    "print(f\"\\nExemple de séquence après padding (premiers 30 éléments):\")\n",
    "print(x_train_padded[0][:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Visualisation des Données\n",
    "\n",
    "Visualisons la distribution des longueurs de séquences et des labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution des longueurs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Distribution des longueurs\n",
    "axes[0].hist(train_lengths, bins=50, alpha=0.7, label='Train', color='skyblue')\n",
    "axes[0].hist(test_lengths, bins=50, alpha=0.7, label='Test', color='lightcoral')\n",
    "axes[0].axvline(MAX_LENGTH, color='red', linestyle='--', linewidth=2, label=f'Max Length ({MAX_LENGTH})')\n",
    "axes[0].set_xlabel('Longueur de la séquence')\n",
    "axes[0].set_ylabel('Fréquence')\n",
    "axes[0].set_title('Distribution des Longueurs de Séquences', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Distribution des labels\n",
    "train_labels_count = pd.Series(y_train).value_counts()\n",
    "test_labels_count = pd.Series(y_test).value_counts()\n",
    "\n",
    "x_pos = np.arange(2)\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x_pos - width/2, [train_labels_count[0], train_labels_count[1]], \n",
    "           width, label='Train', color='skyblue', alpha=0.7)\n",
    "axes[1].bar(x_pos + width/2, [test_labels_count[0], test_labels_count[1]], \n",
    "           width, label='Test', color='lightcoral', alpha=0.7)\n",
    "axes[1].set_xlabel('Label')\n",
    "axes[1].set_ylabel('Nombre d\\'échantillons')\n",
    "axes[1].set_title('Distribution des Labels', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(['Négatif (0)', 'Positif (1)'])\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDistribution des labels d'entraînement:\")\n",
    "print(f\"Négatif (0): {train_labels_count[0]} ({train_labels_count[0]/len(y_train)*100:.1f}%)\")\n",
    "print(f\"Positif (1): {train_labels_count[1]} ({train_labels_count[1]/len(y_train)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construction des Modèles RNN\n",
    "\n",
    "Nous allons créer et comparer trois types de modèles RNN :\n",
    "1. **Simple RNN**\n",
    "2. **LSTM (Long Short-Term Memory)**\n",
    "3. **GRU (Gated Recurrent Unit)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Modèle Simple RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_rnn(embedding_dim=128, rnn_units=64):\n",
    "    \"\"\"\n",
    "    Crée un modèle Simple RNN pour l'analyse de sentiment.\n",
    "    \n",
    "    Architecture:\n",
    "    - Embedding: Convertit les indices en vecteurs denses\n",
    "    - Simple RNN: Traite la séquence\n",
    "    - Dense: Classification finale\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(VOCAB_SIZE, embedding_dim, input_length=MAX_LENGTH),\n",
    "        layers.SimpleRNN(rnn_units, dropout=0.2, recurrent_dropout=0.2),\n",
    "        layers.Dense(1, activation='sigmoid')  # Classification binaire\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Créer le modèle\n",
    "model_simple_rnn = create_simple_rnn()\n",
    "\n",
    "# Compiler le modèle\n",
    "model_simple_rnn.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Afficher l'architecture\n",
    "print(\"Architecture du modèle Simple RNN:\")\n",
    "model_simple_rnn.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Modèle LSTM (Long Short-Term Memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(embedding_dim=128, lstm_units=64):\n",
    "    \"\"\"\n",
    "    Crée un modèle LSTM pour l'analyse de sentiment.\n",
    "    \n",
    "    Architecture:\n",
    "    - Embedding: Convertit les indices en vecteurs denses\n",
    "    - LSTM: Traite la séquence avec mémoire à long terme\n",
    "    - Dense: Classification finale\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(VOCAB_SIZE, embedding_dim, input_length=MAX_LENGTH),\n",
    "        layers.LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2),\n",
    "        layers.Dense(1, activation='sigmoid')  # Classification binaire\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Créer le modèle\n",
    "model_lstm = create_lstm_model()\n",
    "\n",
    "# Compiler le modèle\n",
    "model_lstm.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Afficher l'architecture\n",
    "print(\"Architecture du modèle LSTM:\")\n",
    "model_lstm.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Modèle GRU (Gated Recurrent Unit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gru_model(embedding_dim=128, gru_units=64):\n",
    "    \"\"\"\n",
    "    Crée un modèle GRU pour l'analyse de sentiment.\n",
    "    \n",
    "    Architecture:\n",
    "    - Embedding: Convertit les indices en vecteurs denses\n",
    "    - GRU: Traite la séquence avec portes (simplification de LSTM)\n",
    "    - Dense: Classification finale\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(VOCAB_SIZE, embedding_dim, input_length=MAX_LENGTH),\n",
    "        layers.GRU(gru_units, dropout=0.2, recurrent_dropout=0.2),\n",
    "        layers.Dense(1, activation='sigmoid')  # Classification binaire\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Créer le modèle\n",
    "model_gru = create_gru_model()\n",
    "\n",
    "# Compiler le modèle\n",
    "model_gru.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Afficher l'architecture\n",
    "print(\"Architecture du modèle GRU:\")\n",
    "model_gru.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Modèle LSTM Bidirectionnel (Optionnel - Performance Améliorée)\n",
    "\n",
    "Les LSTM bidirectionnels traitent la séquence dans les deux sens (avant et arrière), ce qui peut améliorer les performances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirectional_lstm(embedding_dim=128, lstm_units=64):\n",
    "    \"\"\"\n",
    "    Crée un modèle LSTM bidirectionnel pour l'analyse de sentiment.\n",
    "    \n",
    "    Architecture:\n",
    "    - Embedding: Convertit les indices en vecteurs denses\n",
    "    - Bidirectional LSTM: Traite la séquence dans les deux sens\n",
    "    - Dense: Classification finale\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(VOCAB_SIZE, embedding_dim, input_length=MAX_LENGTH),\n",
    "        layers.Bidirectional(layers.LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)),\n",
    "        layers.Dense(1, activation='sigmoid')  # Classification binaire\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Créer le modèle\n",
    "model_bidirectional_lstm = create_bidirectional_lstm()\n",
    "\n",
    "# Compiler le modèle\n",
    "model_bidirectional_lstm.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Afficher l'architecture\n",
    "print(\"Architecture du modèle LSTM Bidirectionnel:\")\n",
    "model_bidirectional_lstm.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement des Modèles\n",
    "\n",
    "Nous allons entraîner chaque modèle et comparer leurs performances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'entraînement\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Callbacks pour améliorer l'entraînement\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=0.00001,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Configuration de l'entraînement:\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Validation split: {VALIDATION_SPLIT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pour stocker les historiques\n",
    "histories = {}\n",
    "\n",
    "# Liste des modèles à entraîner\n",
    "models_to_train = {\n",
    "    'Simple RNN': model_simple_rnn,\n",
    "    'LSTM': model_lstm,\n",
    "    'GRU': model_gru,\n",
    "    'Bidirectional LSTM': model_bidirectional_lstm\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement de chaque modèle\n",
    "for model_name, model in models_to_train.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Entraînement du modèle: {model_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Entraîner le modèle\n",
    "    history = model.fit(\n",
    "        x_train_padded, y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Stocker l'historique\n",
    "    histories[model_name] = history\n",
    "    \n",
    "    # Évaluation sur le test set\n",
    "    test_loss, test_accuracy = model.evaluate(x_test_padded, y_test, verbose=0)\n",
    "    print(f\"\\nRésultats sur le test set:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisation des Résultats d'Entraînement\n",
    "\n",
    "Comparons les courbes d'apprentissage de chaque modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(histories_dict):\n",
    "    \"\"\"Visualise l'historique d'entraînement pour tous les modèles\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Loss - Training\n",
    "    axes[0, 0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    for model_name, history in histories_dict.items():\n",
    "        axes[0, 0].plot(history.history['loss'], label=model_name, linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Loss - Validation\n",
    "    axes[0, 1].set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "    for model_name, history in histories_dict.items():\n",
    "        axes[0, 1].plot(history.history['val_loss'], label=model_name, linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # Accuracy - Training\n",
    "    axes[1, 0].set_title('Training Accuracy', fontsize=14, fontweight='bold')\n",
    "    for model_name, history in histories_dict.items():\n",
    "        axes[1, 0].plot(history.history['accuracy'], label=model_name, linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Accuracy - Validation\n",
    "    axes[1, 1].set_title('Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    for model_name, history in histories_dict.items():\n",
    "        axes[1, 1].plot(history.history['val_accuracy'], label=model_name, linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualiser les résultats\n",
    "plot_training_history(histories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparaison des Performances\n",
    "\n",
    "Comparons les performances finales de tous les modèles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation de tous les modèles sur le test set\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models_to_train.items():\n",
    "    test_loss, test_accuracy = model.evaluate(x_test_padded, y_test, verbose=0)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_pred_probs = model.predict(x_test_padded, verbose=0)\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Métriques supplémentaires\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "# Créer un DataFrame pour la comparaison\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.sort_values('test_accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARAISON DES MODÈLES\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Visualisation graphique\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "metrics = ['test_accuracy', 'precision', 'recall', 'f1_score']\n",
    "titles = ['Test Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    axes[row, col].bar(results_df.index, results_df[metric], color='steelblue', alpha=0.7)\n",
    "    axes[row, col].set_title(title, fontsize=14, fontweight='bold')\n",
    "    axes[row, col].set_ylabel(metric.replace('_', ' ').title())\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    axes[row, col].grid(axis='y', alpha=0.3)\n",
    "    axes[row, col].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Matrices de Confusion\n",
    "\n",
    "Visualisons les matrices de confusion pour chaque modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du nombre de lignes et colonnes pour la grille\n",
    "n_models = len(models_to_train)\n",
    "n_cols = 2\n",
    "n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 6*n_rows))\n",
    "axes = axes.flatten() if n_models > 1 else [axes]\n",
    "\n",
    "for idx, (model_name, model) in enumerate(models_to_train.items()):\n",
    "    # Prédictions\n",
    "    y_pred_probs = model.predict(x_test_padded, verbose=0)\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        ax=axes[idx],\n",
    "        xticklabels=['Négatif', 'Positif'],\n",
    "        yticklabels=['Négatif', 'Positif']\n",
    "    )\n",
    "    \n",
    "    accuracy = results[model_name]['test_accuracy']\n",
    "    axes[idx].set_title(f'{model_name}\\nAccuracy: {accuracy:.4f}', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Vraie classe')\n",
    "    axes[idx].set_xlabel('Classe prédite')\n",
    "\n",
    "# Masquer les axes inutilisés\n",
    "for idx in range(n_models, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Rapports de Classification Détaillés\n",
    "\n",
    "Affichons les rapports de classification pour le meilleur modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver le meilleur modèle\n",
    "best_model_name = results_df.index[0]\n",
    "best_model = models_to_train[best_model_name]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Rapport de Classification - {best_model_name}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Prédictions du meilleur modèle\n",
    "y_pred_probs = best_model.predict(x_test_padded, verbose=0)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Rapport de classification\n",
    "print(classification_report(y_test, y_pred, target_names=['Négatif', 'Positif']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prédictions sur Nouvelles Phrases\n",
    "\n",
    "Testons le modèle sur de nouvelles critiques que nous créons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, text, word_index_dict, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Prédit le sentiment d'une nouvelle phrase.\n",
    "    \n",
    "    Args:\n",
    "        model: Modèle entraîné\n",
    "        text: Texte à analyser\n",
    "        word_index_dict: Dictionnaire word_index\n",
    "        max_length: Longueur maximale de la séquence\n",
    "    \n",
    "    Returns:\n",
    "        sentiment: 'Positif' ou 'Négatif'\n",
    "        confidence: Probabilité de confiance\n",
    "    \"\"\"\n",
    "    # Nettoyer le texte\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Convertir en indices\n",
    "    sequence = [word_index_dict.get(word, 2) for word in words]  # 2 = <UNK>\n",
    "    \n",
    "    # Padding\n",
    "    sequence = pad_sequences([sequence], maxlen=max_length, padding='pre', truncating='pre')\n",
    "    \n",
    "    # Prédiction\n",
    "    prediction = model.predict(sequence, verbose=0)[0][0]\n",
    "    sentiment = 'Positif' if prediction > 0.5 else 'Négatif'\n",
    "    confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "    \n",
    "    return sentiment, confidence, prediction\n",
    "\n",
    "# Exemples de nouvelles critiques\n",
    "new_reviews = [\n",
    "    \"This movie was absolutely fantastic! The acting was superb and the plot was engaging.\",\n",
    "    \"I hated this film. It was boring and poorly acted. I want my money back!\",\n",
    "    \"The movie was okay. Nothing special, but not terrible either.\",\n",
    "    \"Brilliant cinematography and excellent performances from all actors. Highly recommend!\",\n",
    "    \"This is the worst movie I have ever seen. Terrible script and bad direction.\",\n",
    "    \"A masterpiece of cinema. The director's vision is remarkable and the story is compelling.\",\n",
    "    \"Not bad, but could have been better. Some good moments but overall disappointing.\",\n",
    "    \"One of the best films of the year! Outstanding storytelling and character development.\"\n",
    "]\n",
    "\n",
    "print(\"Prédictions sur de nouvelles critiques:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, review in enumerate(new_reviews, 1):\n",
    "    sentiment, confidence, prob = predict_sentiment(best_model, review, word_index)\n",
    "    print(f\"\\nCritique {i}:\")\n",
    "    print(f\"Texte: {review[:80]}...\")\n",
    "    print(f\"Sentiment prédit: {sentiment} (Confiance: {confidence:.2%}, Probabilité: {prob:.4f})\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyse Comparative des Architectures RNN\n",
    "\n",
    "Comparons visuellement les performances des différents types de RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un graphique comparatif\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "models_names = list(results_df.index)\n",
    "accuracies = results_df['test_accuracy'].values\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen', 'plum']\n",
    "\n",
    "bars = ax.barh(models_names, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "            f'{acc:.4f} ({acc*100:.2f}%)', \n",
    "            ha='left', va='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.set_xlabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Comparaison des Performances des Modèles RNN', \n",
    "            fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlim([0, 1.05])\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Résumé textuel\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RÉSUMÉ COMPARATIF\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nMeilleur modèle: {best_model_name}\")\n",
    "print(f\"Accuracy: {results[best_model_name]['test_accuracy']:.4f} ({results[best_model_name]['test_accuracy']*100:.2f}%)\")\n",
    "print(f\"\\nClassement des modèles:\")\n",
    "for i, (model_name, metrics) in enumerate(results.items(), 1):\n",
    "    print(f\"{i}. {model_name}: {metrics['test_accuracy']:.4f} ({metrics['test_accuracy']*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sauvegarde du Meilleur Modèle\n",
    "\n",
    "Sauvegardons le meilleur modèle pour une utilisation ultérieure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le meilleur modèle\n",
    "model_filename = f'best_rnn_model_{best_model_name.lower().replace(\" \", \"_\")}.h5'\n",
    "best_model.save(model_filename)\n",
    "print(f\"Modèle sauvegardé: {model_filename}\")\n",
    "\n",
    "# Sauvegarder aussi les poids\n",
    "weights_filename = f'best_rnn_weights_{best_model_name.lower().replace(\" \", \"_\")}.h5'\n",
    "best_model.save_weights(weights_filename)\n",
    "print(f\"Poids sauvegardés: {weights_filename}\")\n",
    "\n",
    "print(\"\\nPour charger le modèle plus tard:\")\n",
    "print(f\"loaded_model = keras.models.load_model('{model_filename}')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusions et Explications\n",
    "\n",
    "### 11.1. Différences entre les Types de RNN\n",
    "\n",
    "**Simple RNN :**\n",
    "- Architecture la plus basique\n",
    "- Souffre du problème du gradient qui disparaît (vanishing gradient)\n",
    "- Limité pour les séquences longues\n",
    "- Plus rapide mais moins performant\n",
    "\n",
    "**LSTM (Long Short-Term Memory) :**\n",
    "- Résout le problème du gradient qui disparaît\n",
    "- Utilise des portes (forget, input, output) pour contrôler l'information\n",
    "- Peut apprendre des dépendances à long terme\n",
    "- Plus de paramètres, donc plus lent mais plus performant\n",
    "\n",
    "**GRU (Gated Recurrent Unit) :**\n",
    "- Compromis entre Simple RNN et LSTM\n",
    "- Moins de paramètres que LSTM (pas de porte output séparée)\n",
    "- Généralement aussi performant que LSTM mais plus rapide\n",
    "- Bon choix pour de nombreux cas d'usage\n",
    "\n",
    "**Bidirectional LSTM :**\n",
    "- Traite la séquence dans les deux sens (avant et arrière)\n",
    "- Capture le contexte complet de la séquence\n",
    "- Généralement le plus performant mais aussi le plus coûteux en calcul\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2. Applications des RNN\n",
    "\n",
    "Les RNN sont utilisés dans de nombreux domaines :\n",
    "\n",
    "1. **Traitement du Langage Naturel (NLP)** :\n",
    "   - Analyse de sentiment\n",
    "   - Traduction automatique\n",
    "   - Génération de texte\n",
    "   - Chatbots\n",
    "\n",
    "2. **Séries Temporelles** :\n",
    "   - Prédiction de cours boursiers\n",
    "   - Prévisions météorologiques\n",
    "   - Analyse de données sensorielles\n",
    "\n",
    "3. **Reconnaissance Vocale** :\n",
    "   - Transcription audio\n",
    "   - Commandes vocales\n",
    "   - Reconnaissance du locuteur\n",
    "\n",
    "4. **Génération de Contenu** :\n",
    "   - Génération de musique\n",
    "   - Génération de texte créatif\n",
    "   - Résumé automatique\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
